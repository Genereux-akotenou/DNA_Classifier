{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c2778f2-e48e-4274-92c1-6fc663121b2b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div style=\"hwidth: 100%; background-color: #ddd; overflow:hidden; \">\n",
    "    <div style=\"display: flex; justify-content: center; align-items: center; border-bottom: 10px solid #80c4e7; padding: 3px;\">\n",
    "        <h2 style=\"position: relative; top: 3px; left: 8px;\">S2 Project: DNA Classification - (part2: Approach 2)</h2>\n",
    "        <img style=\"position: absolute; height: 68px; top: -2px;; right: 18px\" src=\"./Content/Notebook-images/dna1.png\"/>\n",
    "    </div>\n",
    "    <div style=\"padding: 3px 8px;\">\n",
    "        \n",
    "1. **Description**:\n",
    "   - **Idea**: k-mer Representation with Frequency Analysis\n",
    "   - Break the DNA sequence into k-mers (subsequences of length k).\n",
    "   - Perform frequency analysis to create a feature vector based on the occurrence of each k-mer.\n",
    "   - Use this feature vector as input to the model.\n",
    "\n",
    "3. **Pros**:\n",
    "   - Captures local context within each k-mer.\n",
    "   - Simplifies the input representation by reducing it to frequency counts.\n",
    "\n",
    "4. **Cons**:\n",
    "   - Loses positional information beyond the k-mer length.\n",
    "   - Treating it as a [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) model ignores the order of k-mers.\n",
    "\n",
    "    </div>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d538eb-39b2-4c68-97be-0de7f710ea77",
   "metadata": {},
   "source": [
    "### 1 - Importing utils\n",
    "The following code cells will import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d34e581c-3277-4b8c-9457-abb5b1afc43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Metric and utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3107eda7-ba16-446e-8250-9dbd63a52edd",
   "metadata": {},
   "source": [
    "### 2 - Importing Dataset\n",
    "The following function will read our preprocessed **.csv file** and return a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a9cceb-5284-4872-a0f5-b8aef5936f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>length</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AT1G51140.1</td>\n",
       "      <td>AAGTTTCTCTCACGTTCTCTTTTTTAATTTTAATTTCTCGCCGGAA...</td>\n",
       "      <td>2297</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AT1G73830.1</td>\n",
       "      <td>ACTTTCTATTTTCACCAATTTTCAAAAAAAAAATAAAAATTGAAAC...</td>\n",
       "      <td>1473</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AT1G09530.1</td>\n",
       "      <td>AGTTACAGACGATTTGGTCCCCTCTCTTCTCTCTCTGCGTCCGTCT...</td>\n",
       "      <td>2958</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AT1G49770.1</td>\n",
       "      <td>ATGACTAATGCTCAAGAGTTGGGGCAAGAGGGTTTTATGTGGGGCA...</td>\n",
       "      <td>2205</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AT1G68810.1</td>\n",
       "      <td>AAACTTTTGTCTCTTTTTAACTCTCTTAACTTTCGTTTCTTCTCCT...</td>\n",
       "      <td>1998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                           sequence  length  \\\n",
       "0  AT1G51140.1  AAGTTTCTCTCACGTTCTCTTTTTTAATTTTAATTTCTCGCCGGAA...    2297   \n",
       "1  AT1G73830.1  ACTTTCTATTTTCACCAATTTTCAAAAAAAAAATAAAAATTGAAAC...    1473   \n",
       "2  AT1G09530.1  AGTTACAGACGATTTGGTCCCCTCTCTTCTCTCTCTGCGTCCGTCT...    2958   \n",
       "3  AT1G49770.1  ATGACTAATGCTCAAGAGTTGGGGCAAGAGGGTTTTATGTGGGGCA...    2205   \n",
       "4  AT1G68810.1  AAACTTTTGTCTCTTTTTAACTCTCTTAACTTTCGTTTCTTCTCCT...    1998   \n",
       "\n",
       "   class  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"./Output/Arabidopsis_thaliana_GHLH_and_CYP_gene.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e24c4e-4ccb-4283-831b-cca7b9d2e9be",
   "metadata": {},
   "source": [
    "### 3 - Preprocessing\n",
    "Instead of taking each base as an individual feature, we transform DNA sequences using the k-mer representation, a widely adopted method in DNA sequence analysis. The k-mer approach captures richer contextual information for each nucleotide by concatenating it with its subsequent bases to form k-mers. For example, the DNA sequence ‘ATGCCA’ can be tranformed into four 3-mers: \"ATG, TGC, GCC, CCA\", or into three 4-mers: \"ATGC, TGCC, GCCA\". In our experiments, we will try these k-mer length: **3, 4, 5, and 6**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "692238f1-af43-48b3-b65a-bcc4018ade55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils: to count kmer occurence in DNA sequence on compute  frequence\n",
    "\n",
    "def kmer_count(sequence, k=3, step=1):\n",
    "    kmers_count = {}\n",
    "    s = 0\n",
    "    for i in range(0, len(sequence) - k + 1, step):\n",
    "        kmer = sequence[i:i + k]\n",
    "        s += 1\n",
    "        if kmer in kmers_count:\n",
    "            kmers_count[kmer] += 1\n",
    "        else:\n",
    "            kmers_count[kmer] = 1\n",
    "    for key, value in kmers_count.items():\n",
    "        kmers_count[key] = value / s\n",
    "\n",
    "    return kmers_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc72e5ba-bebb-47ff-845c-2f504db3258c",
   "metadata": {},
   "source": [
    "### 4 - Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1866a29-7df5-4f04-9d54-79a34b8afca5",
   "metadata": {},
   "source": [
    "<h4 style=\"background-color: #80c4e6; display: flex;\">\n",
    "    <ul><li>k=3</li></ul>\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5f2ef72-8d7f-4b99-9bd4-ddb834847464",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "sequences   = dataset['sequence']\n",
    "kmers_count = []\n",
    "for i in range(len(sequences)):\n",
    "    kmers_count.append(kmer_count(sequences[i], k=k, step=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad0621c6-3904-40f8-a00f-1610d755efae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAA</th>\n",
       "      <th>AAC</th>\n",
       "      <th>AAG</th>\n",
       "      <th>AAT</th>\n",
       "      <th>ACA</th>\n",
       "      <th>ACC</th>\n",
       "      <th>ACG</th>\n",
       "      <th>ACT</th>\n",
       "      <th>AGA</th>\n",
       "      <th>AGC</th>\n",
       "      <th>...</th>\n",
       "      <th>TCG</th>\n",
       "      <th>TCT</th>\n",
       "      <th>TGA</th>\n",
       "      <th>TGC</th>\n",
       "      <th>TGG</th>\n",
       "      <th>TGT</th>\n",
       "      <th>TTA</th>\n",
       "      <th>TTC</th>\n",
       "      <th>TTG</th>\n",
       "      <th>TTT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.028758</td>\n",
       "      <td>0.018736</td>\n",
       "      <td>0.028322</td>\n",
       "      <td>0.024401</td>\n",
       "      <td>0.013508</td>\n",
       "      <td>0.008715</td>\n",
       "      <td>0.005664</td>\n",
       "      <td>0.018301</td>\n",
       "      <td>0.029630</td>\n",
       "      <td>0.012636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010022</td>\n",
       "      <td>0.025272</td>\n",
       "      <td>0.026144</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>0.016993</td>\n",
       "      <td>0.018301</td>\n",
       "      <td>0.024837</td>\n",
       "      <td>0.026144</td>\n",
       "      <td>0.043573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.042148</td>\n",
       "      <td>0.015636</td>\n",
       "      <td>0.020394</td>\n",
       "      <td>0.040789</td>\n",
       "      <td>0.014956</td>\n",
       "      <td>0.004759</td>\n",
       "      <td>0.005438</td>\n",
       "      <td>0.015636</td>\n",
       "      <td>0.031271</td>\n",
       "      <td>0.007478</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010197</td>\n",
       "      <td>0.035350</td>\n",
       "      <td>0.019714</td>\n",
       "      <td>0.008158</td>\n",
       "      <td>0.010877</td>\n",
       "      <td>0.010197</td>\n",
       "      <td>0.031271</td>\n",
       "      <td>0.036030</td>\n",
       "      <td>0.012916</td>\n",
       "      <td>0.062542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.030108</td>\n",
       "      <td>0.019959</td>\n",
       "      <td>0.025034</td>\n",
       "      <td>0.021313</td>\n",
       "      <td>0.013532</td>\n",
       "      <td>0.008119</td>\n",
       "      <td>0.007442</td>\n",
       "      <td>0.016915</td>\n",
       "      <td>0.024357</td>\n",
       "      <td>0.011502</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010487</td>\n",
       "      <td>0.032476</td>\n",
       "      <td>0.019959</td>\n",
       "      <td>0.015223</td>\n",
       "      <td>0.017591</td>\n",
       "      <td>0.022327</td>\n",
       "      <td>0.016238</td>\n",
       "      <td>0.029770</td>\n",
       "      <td>0.026725</td>\n",
       "      <td>0.046008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.037222</td>\n",
       "      <td>0.016795</td>\n",
       "      <td>0.019065</td>\n",
       "      <td>0.029051</td>\n",
       "      <td>0.018157</td>\n",
       "      <td>0.009532</td>\n",
       "      <td>0.005447</td>\n",
       "      <td>0.011802</td>\n",
       "      <td>0.022696</td>\n",
       "      <td>0.006809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005901</td>\n",
       "      <td>0.024966</td>\n",
       "      <td>0.019065</td>\n",
       "      <td>0.012710</td>\n",
       "      <td>0.011348</td>\n",
       "      <td>0.020881</td>\n",
       "      <td>0.031321</td>\n",
       "      <td>0.027236</td>\n",
       "      <td>0.019519</td>\n",
       "      <td>0.059464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.034068</td>\n",
       "      <td>0.019539</td>\n",
       "      <td>0.033567</td>\n",
       "      <td>0.018537</td>\n",
       "      <td>0.022044</td>\n",
       "      <td>0.011523</td>\n",
       "      <td>0.009519</td>\n",
       "      <td>0.012024</td>\n",
       "      <td>0.036072</td>\n",
       "      <td>0.014028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009018</td>\n",
       "      <td>0.028056</td>\n",
       "      <td>0.017535</td>\n",
       "      <td>0.006012</td>\n",
       "      <td>0.013026</td>\n",
       "      <td>0.019539</td>\n",
       "      <td>0.018537</td>\n",
       "      <td>0.019539</td>\n",
       "      <td>0.020541</td>\n",
       "      <td>0.049098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        AAA       AAC       AAG       AAT       ACA       ACC       ACG  \\\n",
       "0  0.028758  0.018736  0.028322  0.024401  0.013508  0.008715  0.005664   \n",
       "1  0.042148  0.015636  0.020394  0.040789  0.014956  0.004759  0.005438   \n",
       "2  0.030108  0.019959  0.025034  0.021313  0.013532  0.008119  0.007442   \n",
       "3  0.037222  0.016795  0.019065  0.029051  0.018157  0.009532  0.005447   \n",
       "4  0.034068  0.019539  0.033567  0.018537  0.022044  0.011523  0.009519   \n",
       "\n",
       "        ACT       AGA       AGC  ...       TCG       TCT       TGA       TGC  \\\n",
       "0  0.018301  0.029630  0.012636  ...  0.010022  0.025272  0.026144  0.013072   \n",
       "1  0.015636  0.031271  0.007478  ...  0.010197  0.035350  0.019714  0.008158   \n",
       "2  0.016915  0.024357  0.011502  ...  0.010487  0.032476  0.019959  0.015223   \n",
       "3  0.011802  0.022696  0.006809  ...  0.005901  0.024966  0.019065  0.012710   \n",
       "4  0.012024  0.036072  0.014028  ...  0.009018  0.028056  0.017535  0.006012   \n",
       "\n",
       "        TGG       TGT       TTA       TTC       TTG       TTT  \n",
       "0  0.014815  0.016993  0.018301  0.024837  0.026144  0.043573  \n",
       "1  0.010877  0.010197  0.031271  0.036030  0.012916  0.062542  \n",
       "2  0.017591  0.022327  0.016238  0.029770  0.026725  0.046008  \n",
       "3  0.011348  0.020881  0.031321  0.027236  0.019519  0.059464  \n",
       "4  0.013026  0.019539  0.018537  0.019539  0.020541  0.049098  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = DictVectorizer(sparse=False)\n",
    "feature_values = v.fit_transform(kmers_count)\n",
    "feature_names = v.get_feature_names_out()\n",
    "X_3 = pd.DataFrame(feature_values, columns=feature_names)\n",
    "X_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6cbc5c6-e969-471f-bc78-2a0391af8261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = dataset['class']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d00fa3c-5fe5-4707-b1c8-ad810cd28a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of train/test splits:\n",
      "X_train: (304, 64)\n",
      "X_test: (76, 64)\n",
      "y_train: (304,)\n",
      "y_test: (76,)\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "X_3_train, X_3_test, y_train, y_test = train_test_split(X_3, y, train_size=0.8, stratify=y)\n",
    "\n",
    "print(\"Shapes of train/test splits:\")\n",
    "print(\"X_train:\", X_3_train.shape)\n",
    "print(\"X_test:\", X_3_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b82f5e9-04cf-48a8-ac3e-935a02b77e50",
   "metadata": {},
   "source": [
    "* Let tune our differents model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08195d62-5519-4a02-bc55-b86486b291b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grids\n",
    "param_grids = {\n",
    "    \"Nearest Neighbors\": {'n_neighbors': [3, 5, 7]},\n",
    "    \"Gaussian Process\": {'kernel': [1.0 * RBF(1.0), 1.0 * RBF(0.5), 1.0 * RBF(2.0)]},\n",
    "    \"Random Forest\": {'max_depth': [3, 5, 7], 'n_estimators': [10, 50, 100], 'max_features': [1, 2, 3]},\n",
    "    \"Neural Net\": {'alpha': [0.0001, 0.001, 0.01]},\n",
    "    \"AdaBoost\": {'n_estimators': [50, 100, 200]},\n",
    "    \"Naive Bayes\": {},\n",
    "    \"SVM Linear\": {'C': [0.1, 1, 10]},\n",
    "    \"SVM RBF\": {'C': [0.1, 1, 10], 'gamma': [0.001, 0.01, 0.1]},\n",
    "    \"MultinomialNB\": {'alpha': [0.01, 0.001, 0.1, 1, 10]},\n",
    "    \"XGBClassifier\": {'n_estimators':[2, 3, 5, 10, 20, 100, 200], 'max_depth':[2, 3, 5, 7]}\n",
    "}\n",
    "\n",
    "# Models\n",
    "names = [\"Nearest Neighbors\", \"XGBClassifier\", \"Gaussian Process\", \"Random Forest\", \"Neural Net\", \"AdaBoost\", \"Naive Bayes\", \"SVM Linear\", \"SVM RBF\", \"MultinomialNB\"]\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    XGBClassifier(objective='binary:logistic'),\n",
    "    GaussianProcessClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    MLPClassifier(max_iter=10000, early_stopping=False),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    SVC(kernel='linear'),\n",
    "    SVC(kernel='rbf'),\n",
    "    MultinomialNB(),\n",
    "]\n",
    "models = zip(names, classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b567f87-bc66-4eff-96fb-9784ef5b2b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Nearest Neighbors...\n",
      "Best params found:  {'n_neighbors': 3}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'Nearest Neighbors' - acc: 0.7996774193548387 ±(0.04596582401834304)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'Nearest Neighbors' - acc : 0.8289473684210527\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.66      0.76        32\n",
      "           1       0.79      0.95      0.87        44\n",
      "\n",
      "    accuracy                           0.83        76\n",
      "   macro avg       0.85      0.81      0.81        76\n",
      "weighted avg       0.84      0.83      0.82        76\n",
      "\n",
      "Processing XGBClassifier...\n",
      "Best params found:  {'max_depth': 7, 'n_estimators': 100}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'XGBClassifier' - acc: 0.8490322580645161 ±(0.06368315313742923)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'XGBClassifier' - acc : 0.8947368421052632\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.84      0.87        32\n",
      "           1       0.89      0.93      0.91        44\n",
      "\n",
      "    accuracy                           0.89        76\n",
      "   macro avg       0.90      0.89      0.89        76\n",
      "weighted avg       0.89      0.89      0.89        76\n",
      "\n",
      "Processing Gaussian Process...\n",
      "Best params found:  {'kernel': 1**2 * RBF(length_scale=1)}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'Gaussian Process' - acc: 0.8787096774193548 ±(0.052381817273740375)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'Gaussian Process' - acc : 0.9210526315789473\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91        32\n",
      "           1       0.93      0.93      0.93        44\n",
      "\n",
      "    accuracy                           0.92        76\n",
      "   macro avg       0.92      0.92      0.92        76\n",
      "weighted avg       0.92      0.92      0.92        76\n",
      "\n",
      "Processing Random Forest...\n",
      "Best params found:  {'max_depth': 7, 'max_features': 3, 'n_estimators': 50}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'Random Forest' - acc: 0.8255913978494623 ±(0.05596523705053278)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'Random Forest' - acc : 0.8947368421052632\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.81      0.87        32\n",
      "           1       0.88      0.95      0.91        44\n",
      "\n",
      "    accuracy                           0.89        76\n",
      "   macro avg       0.90      0.88      0.89        76\n",
      "weighted avg       0.90      0.89      0.89        76\n",
      "\n",
      "Processing Neural Net...\n",
      "Best params found:  {'alpha': 0.001}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'Neural Net' - acc: 0.8626881720430107 ±(0.06177332789945339)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'Neural Net' - acc : 0.868421052631579\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84        32\n",
      "           1       0.87      0.91      0.89        44\n",
      "\n",
      "    accuracy                           0.87        76\n",
      "   macro avg       0.87      0.86      0.86        76\n",
      "weighted avg       0.87      0.87      0.87        76\n",
      "\n",
      "Processing AdaBoost...\n",
      "Best params found:  {'n_estimators': 200}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'AdaBoost' - acc: 0.8193548387096774 ±(0.03221502505131013)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'AdaBoost' - acc : 0.881578947368421\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.88      0.86        32\n",
      "           1       0.91      0.89      0.90        44\n",
      "\n",
      "    accuracy                           0.88        76\n",
      "   macro avg       0.88      0.88      0.88        76\n",
      "weighted avg       0.88      0.88      0.88        76\n",
      "\n",
      "Processing Naive Bayes...\n",
      "Best params found:  {}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'Naive Bayes' - acc: 0.8124731182795699 ±(0.051568529311980846)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'Naive Bayes' - acc : 0.8157894736842105\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78        32\n",
      "           1       0.84      0.84      0.84        44\n",
      "\n",
      "    accuracy                           0.82        76\n",
      "   macro avg       0.81      0.81      0.81        76\n",
      "weighted avg       0.82      0.82      0.82        76\n",
      "\n",
      "Processing SVM Linear...\n",
      "Best params found:  {'C': 0.1}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'SVM Linear' - acc: 0.5716129032258065 ±(0.08593570990315456)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'SVM Linear' - acc : 0.5789473684210527\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        32\n",
      "           1       0.58      1.00      0.73        44\n",
      "\n",
      "    accuracy                           0.58        76\n",
      "   macro avg       0.29      0.50      0.37        76\n",
      "weighted avg       0.34      0.58      0.42        76\n",
      "\n",
      "Processing SVM RBF...\n",
      "Best params found:  {'C': 0.1, 'gamma': 0.001}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'SVM RBF' - acc: 0.5716129032258065 ±(0.08593570990315456)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'SVM RBF' - acc : 0.5789473684210527\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        32\n",
      "           1       0.58      1.00      0.73        44\n",
      "\n",
      "    accuracy                           0.58        76\n",
      "   macro avg       0.29      0.50      0.37        76\n",
      "weighted avg       0.34      0.58      0.42        76\n",
      "\n",
      "Processing MultinomialNB...\n",
      "Best params found:  {'alpha': 0.01}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'MultinomialNB' - acc: 0.5716129032258065 ±(0.08593570990315456)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'MultinomialNB' - acc : 0.5789473684210527\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        32\n",
      "           1       0.58      1.00      0.73        44\n",
      "\n",
      "    accuracy                           0.58        76\n",
      "   macro avg       0.29      0.50      0.37        76\n",
      "weighted avg       0.34      0.58      0.42        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "names   = []\n",
    "best_parameters = []\n",
    "\n",
    "for name, model in models:\n",
    "    print(f\"Processing {name}...\")\n",
    "    param_grid = param_grids[name]\n",
    "    kfold = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "    \n",
    "    # Perform grid search & Get the best model\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=kfold, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_3_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    best_parameters.append((name, best_params))\n",
    "    print('Best params found: ', best_params)\n",
    "    \n",
    "    # Cross-validation results\n",
    "    cv_results = cross_val_score(best_model, X_3_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"[Train] - '{}' - acc: {} ±({})\".format(name, cv_results.mean(), cv_results.std())\n",
    "    print(\"{}\\n{}\\n{}\".format('-'*80, msg, '-'*80))\n",
    "    \n",
    "    # Fit the best model\n",
    "    best_model.fit(X_3_train, y_train)\n",
    "    \n",
    "    # Make predictions and print test results\n",
    "    predictions = best_model.predict(X_3_test)\n",
    "    print(\"[Test]  - '{}' - acc : {}\".format(name, accuracy_score(y_test, predictions)))\n",
    "    print(classification_report(y_test, predictions), end=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabf6d66-751b-4e23-a786-5625a29f515d",
   "metadata": {},
   "source": [
    "<h4 style=\"background-color: #80c4e6; display: flex;\">\n",
    "    <ul><li>k=4</li></ul>\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52c95884-ba27-4534-ad6b-c4aff1dbdcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "sequences   = dataset['sequence']\n",
    "kmers_count = []\n",
    "for i in range(len(sequences)):\n",
    "    kmers_count.append(kmer_count(sequences[i], k=k, step=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e123505c-7efb-48e5-b184-ff314fd31e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAAA</th>\n",
       "      <th>AAAC</th>\n",
       "      <th>AAAG</th>\n",
       "      <th>AAAT</th>\n",
       "      <th>AACA</th>\n",
       "      <th>AACC</th>\n",
       "      <th>AACG</th>\n",
       "      <th>AACT</th>\n",
       "      <th>AAGA</th>\n",
       "      <th>AAGC</th>\n",
       "      <th>...</th>\n",
       "      <th>TTCG</th>\n",
       "      <th>TTCT</th>\n",
       "      <th>TTGA</th>\n",
       "      <th>TTGC</th>\n",
       "      <th>TTGG</th>\n",
       "      <th>TTGT</th>\n",
       "      <th>TTTA</th>\n",
       "      <th>TTTC</th>\n",
       "      <th>TTTG</th>\n",
       "      <th>TTTT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.008282</td>\n",
       "      <td>0.008718</td>\n",
       "      <td>0.006975</td>\n",
       "      <td>0.004795</td>\n",
       "      <td>0.006103</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.007411</td>\n",
       "      <td>0.011334</td>\n",
       "      <td>0.005667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.009590</td>\n",
       "      <td>0.007411</td>\n",
       "      <td>0.004795</td>\n",
       "      <td>0.006103</td>\n",
       "      <td>0.007847</td>\n",
       "      <td>0.006975</td>\n",
       "      <td>0.008718</td>\n",
       "      <td>0.010026</td>\n",
       "      <td>0.017873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.019728</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>0.014966</td>\n",
       "      <td>0.004082</td>\n",
       "      <td>0.002041</td>\n",
       "      <td>0.002041</td>\n",
       "      <td>0.007483</td>\n",
       "      <td>0.010884</td>\n",
       "      <td>0.002041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>0.017007</td>\n",
       "      <td>0.005442</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>0.002041</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>0.010884</td>\n",
       "      <td>0.019048</td>\n",
       "      <td>0.006803</td>\n",
       "      <td>0.025850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.007783</td>\n",
       "      <td>0.007783</td>\n",
       "      <td>0.010152</td>\n",
       "      <td>0.004399</td>\n",
       "      <td>0.007107</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>0.002707</td>\n",
       "      <td>0.006430</td>\n",
       "      <td>0.010829</td>\n",
       "      <td>0.005076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004061</td>\n",
       "      <td>0.013875</td>\n",
       "      <td>0.008122</td>\n",
       "      <td>0.004399</td>\n",
       "      <td>0.005076</td>\n",
       "      <td>0.009137</td>\n",
       "      <td>0.007107</td>\n",
       "      <td>0.010491</td>\n",
       "      <td>0.011168</td>\n",
       "      <td>0.017259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.015441</td>\n",
       "      <td>0.005450</td>\n",
       "      <td>0.006812</td>\n",
       "      <td>0.009537</td>\n",
       "      <td>0.006358</td>\n",
       "      <td>0.003633</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.005450</td>\n",
       "      <td>0.009083</td>\n",
       "      <td>0.003179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002271</td>\n",
       "      <td>0.014078</td>\n",
       "      <td>0.004087</td>\n",
       "      <td>0.004541</td>\n",
       "      <td>0.004087</td>\n",
       "      <td>0.006812</td>\n",
       "      <td>0.013170</td>\n",
       "      <td>0.012262</td>\n",
       "      <td>0.009083</td>\n",
       "      <td>0.024977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011529</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>0.010025</td>\n",
       "      <td>0.006516</td>\n",
       "      <td>0.009023</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.004511</td>\n",
       "      <td>0.016040</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002005</td>\n",
       "      <td>0.011028</td>\n",
       "      <td>0.004511</td>\n",
       "      <td>0.002005</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>0.008020</td>\n",
       "      <td>0.007018</td>\n",
       "      <td>0.008521</td>\n",
       "      <td>0.011028</td>\n",
       "      <td>0.022556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       AAAA      AAAC      AAAG      AAAT      AACA      AACC      AACG  \\\n",
       "0  0.008282  0.008718  0.006975  0.004795  0.006103  0.003487  0.001744   \n",
       "1  0.019728  0.004762  0.002721  0.014966  0.004082  0.002041  0.002041   \n",
       "2  0.007783  0.007783  0.010152  0.004399  0.007107  0.003723  0.002707   \n",
       "3  0.015441  0.005450  0.006812  0.009537  0.006358  0.003633  0.001362   \n",
       "4  0.011529  0.006015  0.010025  0.006516  0.009023  0.002506  0.003509   \n",
       "\n",
       "       AACT      AAGA      AAGC  ...      TTCG      TTCT      TTGA      TTGC  \\\n",
       "0  0.007411  0.011334  0.005667  ...  0.003923  0.009590  0.007411  0.004795   \n",
       "1  0.007483  0.010884  0.002041  ...  0.003401  0.017007  0.005442  0.002721   \n",
       "2  0.006430  0.010829  0.005076  ...  0.004061  0.013875  0.008122  0.004399   \n",
       "3  0.005450  0.009083  0.003179  ...  0.002271  0.014078  0.004087  0.004541   \n",
       "4  0.004511  0.016040  0.007519  ...  0.002005  0.011028  0.004511  0.002005   \n",
       "\n",
       "       TTGG      TTGT      TTTA      TTTC      TTTG      TTTT  \n",
       "0  0.006103  0.007847  0.006975  0.008718  0.010026  0.017873  \n",
       "1  0.002041  0.002721  0.010884  0.019048  0.006803  0.025850  \n",
       "2  0.005076  0.009137  0.007107  0.010491  0.011168  0.017259  \n",
       "3  0.004087  0.006812  0.013170  0.012262  0.009083  0.024977  \n",
       "4  0.006015  0.008020  0.007018  0.008521  0.011028  0.022556  \n",
       "\n",
       "[5 rows x 256 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = DictVectorizer(sparse=False)\n",
    "feature_values = v.fit_transform(kmers_count)\n",
    "feature_names = v.get_feature_names_out()\n",
    "X_4 = pd.DataFrame(feature_values, columns=feature_names)\n",
    "X_4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d61e021-9527-4c80-b58f-0f102254cfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of train/test splits:\n",
      "X_train: (304, 256)\n",
      "X_test: (76, 256)\n",
      "y_train: (304,)\n",
      "y_test: (76,)\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "X_4_train, X_4_test, y_train, y_test = train_test_split(X_4, y, train_size=0.8, stratify=y)\n",
    "\n",
    "print(\"Shapes of train/test splits:\")\n",
    "print(\"X_train:\", X_4_train.shape)\n",
    "print(\"X_test:\", X_4_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a53752-dedd-4db8-80a4-2c0f31e9d503",
   "metadata": {},
   "source": [
    "* Let tune our differents model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daa157ff-5ec7-4ad4-a826-56f1d375ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grids\n",
    "param_grids = {\n",
    "    \"Nearest Neighbors\": {'n_neighbors': [3, 5, 7]},\n",
    "    \"Gaussian Process\": {'kernel': [1.0 * RBF(1.0), 1.0 * RBF(0.5), 1.0 * RBF(2.0)]},\n",
    "    \"Random Forest\": {'max_depth': [3, 5, 7], 'n_estimators': [10, 50, 100], 'max_features': [1, 2, 3]},\n",
    "    \"Neural Net\": {'alpha': [0.0001, 0.001, 0.01]},\n",
    "    \"AdaBoost\": {'n_estimators': [50, 100, 200]},\n",
    "    \"Naive Bayes\": {},\n",
    "    \"SVM Linear\": {'C': [0.1, 1, 10]},\n",
    "    \"SVM RBF\": {'C': [0.1, 1, 10], 'gamma': [0.001, 0.01, 0.1]},\n",
    "    \"MultinomialNB\": {'alpha': [0.01, 0.001, 0.1, 1, 10]},\n",
    "    \"XGBClassifier\": {'n_estimators':[2, 3, 5, 10, 20, 100, 200], 'max_depth':[2, 3, 5, 7]}\n",
    "}\n",
    "\n",
    "# Models\n",
    "names = [\"Nearest Neighbors\", \"XGBClassifier\", \"Gaussian Process\", \"Random Forest\", \"Neural Net\", \"AdaBoost\", \"Naive Bayes\", \"SVM Linear\", \"SVM RBF\", \"MultinomialNB\"]\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    XGBClassifier(objective='binary:logistic'),\n",
    "    GaussianProcessClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    MLPClassifier(max_iter=10000, early_stopping=False),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    SVC(kernel='linear'),\n",
    "    SVC(kernel='rbf'),\n",
    "    MultinomialNB()\n",
    "]\n",
    "models = zip(names, classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a694909-4531-42ce-90aa-2a825cde7c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing XGBClassifier...\n",
      "Best params found:  {'max_depth': 3, 'n_estimators': 100}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'XGBClassifier' - acc: 0.8882795698924731 ±(0.03658901372735434)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'XGBClassifier' - acc : 0.881578947368421\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.81      0.85        32\n",
      "           1       0.87      0.93      0.90        44\n",
      "\n",
      "    accuracy                           0.88        76\n",
      "   macro avg       0.88      0.87      0.88        76\n",
      "weighted avg       0.88      0.88      0.88        76\n",
      "\n",
      "Processing Gaussian Process...\n",
      "Best params found:  {'kernel': 1**2 * RBF(length_scale=1)}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'Gaussian Process' - acc: 0.9146236559139785 ±(0.04180901801937289)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'Gaussian Process' - acc : 0.881578947368421\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.78      0.85        32\n",
      "           1       0.86      0.95      0.90        44\n",
      "\n",
      "    accuracy                           0.88        76\n",
      "   macro avg       0.89      0.87      0.88        76\n",
      "weighted avg       0.89      0.88      0.88        76\n",
      "\n",
      "Processing Random Forest...\n",
      "Best params found:  {'max_depth': 7, 'max_features': 3, 'n_estimators': 100}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'Random Forest' - acc: 0.8489247311827957 ±(0.06585571632649272)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'Random Forest' - acc : 0.7763157894736842\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.56      0.68        32\n",
      "           1       0.75      0.93      0.83        44\n",
      "\n",
      "    accuracy                           0.78        76\n",
      "   macro avg       0.80      0.75      0.75        76\n",
      "weighted avg       0.79      0.78      0.77        76\n",
      "\n",
      "Processing Neural Net...\n",
      "Best params found:  {'alpha': 0.01}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'Neural Net' - acc: 0.9081720430107527 ±(0.054188001081506836)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'Neural Net' - acc : 0.8947368421052632\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.81      0.87        32\n",
      "           1       0.88      0.95      0.91        44\n",
      "\n",
      "    accuracy                           0.89        76\n",
      "   macro avg       0.90      0.88      0.89        76\n",
      "weighted avg       0.90      0.89      0.89        76\n",
      "\n",
      "Processing AdaBoost...\n",
      "Best params found:  {'n_estimators': 200}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'AdaBoost' - acc: 0.8816129032258064 ±(0.05176378037065173)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'AdaBoost' - acc : 0.8947368421052632\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.81      0.87        32\n",
      "           1       0.88      0.95      0.91        44\n",
      "\n",
      "    accuracy                           0.89        76\n",
      "   macro avg       0.90      0.88      0.89        76\n",
      "weighted avg       0.90      0.89      0.89        76\n",
      "\n",
      "Processing Naive Bayes...\n",
      "Best params found:  {}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'Naive Bayes' - acc: 0.8416129032258066 ±(0.06869958025662673)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'Naive Bayes' - acc : 0.8289473684210527\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.75      0.79        32\n",
      "           1       0.83      0.89      0.86        44\n",
      "\n",
      "    accuracy                           0.83        76\n",
      "   macro avg       0.83      0.82      0.82        76\n",
      "weighted avg       0.83      0.83      0.83        76\n",
      "\n",
      "Processing SVM Linear...\n",
      "Best params found:  {'C': 0.1}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'SVM Linear' - acc: 0.5723655913978494 ±(0.05402313623317357)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'SVM Linear' - acc : 0.5789473684210527\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        32\n",
      "           1       0.58      1.00      0.73        44\n",
      "\n",
      "    accuracy                           0.58        76\n",
      "   macro avg       0.29      0.50      0.37        76\n",
      "weighted avg       0.34      0.58      0.42        76\n",
      "\n",
      "Processing SVM RBF...\n",
      "Best params found:  {'C': 0.1, 'gamma': 0.001}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'SVM RBF' - acc: 0.5723655913978494 ±(0.05402313623317357)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'SVM RBF' - acc : 0.5789473684210527\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        32\n",
      "           1       0.58      1.00      0.73        44\n",
      "\n",
      "    accuracy                           0.58        76\n",
      "   macro avg       0.29      0.50      0.37        76\n",
      "weighted avg       0.34      0.58      0.42        76\n",
      "\n",
      "Processing MultinomialNB...\n",
      "Best params found:  {'alpha': 0.01}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'MultinomialNB' - acc: 0.5723655913978494 ±(0.05402313623317357)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'MultinomialNB' - acc : 0.5789473684210527\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        32\n",
      "           1       0.58      1.00      0.73        44\n",
      "\n",
      "    accuracy                           0.58        76\n",
      "   macro avg       0.29      0.50      0.37        76\n",
      "weighted avg       0.34      0.58      0.42        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "names   = []\n",
    "best_parameters = []\n",
    "\n",
    "for name, model in models:\n",
    "    print(f\"Processing {name}...\")\n",
    "    param_grid = param_grids[name]\n",
    "    kfold = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "    \n",
    "    # Perform grid search & Get the best model\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=kfold, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_4_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    best_parameters.append((name, best_params))\n",
    "    print('Best params found: ', best_params)\n",
    "    \n",
    "    # Cross-validation results\n",
    "    cv_results = cross_val_score(best_model, X_4_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"[Train] - '{}' - acc: {} ±({})\".format(name, cv_results.mean(), cv_results.std())\n",
    "    print(\"{}\\n{}\\n{}\".format('-'*80, msg, '-'*80))\n",
    "    \n",
    "    # Fit the best model\n",
    "    best_model.fit(X_4_train, y_train)\n",
    "    \n",
    "    # Make predictions and print test results\n",
    "    predictions = best_model.predict(X_4_test)\n",
    "    print(\"[Test]  - '{}' - acc : {}\".format(name, accuracy_score(y_test, predictions)))\n",
    "    print(classification_report(y_test, predictions), end=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24285166-e024-45bc-b64d-f0cab8aa5235",
   "metadata": {},
   "source": [
    "<h4 style=\"background-color: #80c4e6; display: flex;\">\n",
    "    <ul><li>k=5</li></ul>\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fb67668-7b58-435f-bbf1-48a186b7681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "sequences   = dataset['sequence']\n",
    "kmers_count = []\n",
    "for i in range(len(sequences)):\n",
    "    kmers_count.append(kmer_count(sequences[i], k=k, step=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7de95921-e80c-4463-97d9-785d2cf82383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAAAA</th>\n",
       "      <th>AAAAC</th>\n",
       "      <th>AAAAG</th>\n",
       "      <th>AAAAT</th>\n",
       "      <th>AAACA</th>\n",
       "      <th>AAACC</th>\n",
       "      <th>AAACG</th>\n",
       "      <th>AAACT</th>\n",
       "      <th>AAAGA</th>\n",
       "      <th>AAAGC</th>\n",
       "      <th>...</th>\n",
       "      <th>TTTCG</th>\n",
       "      <th>TTTCT</th>\n",
       "      <th>TTTGA</th>\n",
       "      <th>TTTGC</th>\n",
       "      <th>TTTGG</th>\n",
       "      <th>TTTGT</th>\n",
       "      <th>TTTTA</th>\n",
       "      <th>TTTTC</th>\n",
       "      <th>TTTTG</th>\n",
       "      <th>TTTTT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003053</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.003053</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.002617</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.003489</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.003925</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>0.003925</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>0.004361</td>\n",
       "      <td>0.002617</td>\n",
       "      <td>0.005233</td>\n",
       "      <td>0.005669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.002042</td>\n",
       "      <td>0.002042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.009530</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>0.002042</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>0.002042</td>\n",
       "      <td>0.004765</td>\n",
       "      <td>0.007488</td>\n",
       "      <td>0.003404</td>\n",
       "      <td>0.010211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002031</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>0.002031</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.003385</td>\n",
       "      <td>0.004401</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.006432</td>\n",
       "      <td>0.003724</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>0.004401</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>0.003724</td>\n",
       "      <td>0.004739</td>\n",
       "      <td>0.006432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008632</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0.003635</td>\n",
       "      <td>0.001817</td>\n",
       "      <td>0.001817</td>\n",
       "      <td>0.001817</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0.004089</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0.006361</td>\n",
       "      <td>0.001817</td>\n",
       "      <td>0.003635</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>0.007269</td>\n",
       "      <td>0.006361</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>0.009087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004514</td>\n",
       "      <td>0.003009</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.003009</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.003009</td>\n",
       "      <td>0.004012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.004514</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.004012</td>\n",
       "      <td>0.004514</td>\n",
       "      <td>0.004514</td>\n",
       "      <td>0.003009</td>\n",
       "      <td>0.004012</td>\n",
       "      <td>0.011033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      AAAAA     AAAAC     AAAAG     AAAAT     AAACA     AAACC     AAACG  \\\n",
       "0  0.003053  0.000872  0.003053  0.001308  0.002617  0.002181  0.000436   \n",
       "1  0.008850  0.001361  0.000681  0.008850  0.001361  0.000681  0.000681   \n",
       "2  0.002031  0.001354  0.002370  0.002031  0.002370  0.001693  0.000339   \n",
       "3  0.008632  0.001363  0.003635  0.001817  0.001817  0.001817  0.000454   \n",
       "4  0.004514  0.003009  0.002006  0.002006  0.003009  0.000502  0.000502   \n",
       "\n",
       "      AAACT     AAAGA     AAAGC  ...     TTTCG     TTTCT     TTTGA     TTTGC  \\\n",
       "0  0.003489  0.002181  0.001744  ...  0.001308  0.003925  0.001744  0.002181   \n",
       "1  0.002042  0.002042  0.000000  ...  0.000681  0.009530  0.001361  0.002042   \n",
       "2  0.003385  0.004401  0.001693  ...  0.000677  0.006432  0.003724  0.000677   \n",
       "3  0.001363  0.004089  0.000909  ...  0.001363  0.006361  0.001817  0.003635   \n",
       "4  0.002006  0.003009  0.004012  ...  0.001003  0.004514  0.001505  0.001003   \n",
       "\n",
       "      TTTGG     TTTGT     TTTTA     TTTTC     TTTTG     TTTTT  \n",
       "0  0.003925  0.002181  0.004361  0.002617  0.005233  0.005669  \n",
       "1  0.001361  0.002042  0.004765  0.007488  0.003404  0.010211  \n",
       "2  0.002370  0.004401  0.002370  0.003724  0.004739  0.006432  \n",
       "3  0.001363  0.002272  0.007269  0.006361  0.002272  0.009087  \n",
       "4  0.004012  0.004514  0.004514  0.003009  0.004012  0.011033  \n",
       "\n",
       "[5 rows x 1024 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = DictVectorizer(sparse=False)\n",
    "feature_values = v.fit_transform(kmers_count)\n",
    "feature_names = v.get_feature_names_out()\n",
    "X_5 = pd.DataFrame(feature_values, columns=feature_names)\n",
    "X_5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ad01f6d-5e65-4145-9e65-ed5850f3b8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of train/test splits:\n",
      "X_train: (304, 1024)\n",
      "X_test: (76, 1024)\n",
      "y_train: (304,)\n",
      "y_test: (76,)\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "X_5_train, X_5_test, y_train, y_test = train_test_split(X_5, y, train_size=0.8, stratify=y)\n",
    "\n",
    "print(\"Shapes of train/test splits:\")\n",
    "print(\"X_train:\", X_5_train.shape)\n",
    "print(\"X_test:\", X_5_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c31ba62-8507-452e-99db-bf766021823b",
   "metadata": {},
   "source": [
    "* Let tune our differents model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8842303f-99f0-4c09-96cf-c21f4320fda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grids\n",
    "param_grids = {\n",
    "    \"Nearest Neighbors\": {'n_neighbors': [3, 5, 7]},\n",
    "    \"Gaussian Process\": {'kernel': [1.0 * RBF(1.0), 1.0 * RBF(0.5), 1.0 * RBF(2.0)]},\n",
    "    \"Random Forest\": {'max_depth': [3, 5, 7], 'n_estimators': [10, 50, 100], 'max_features': [1, 2, 3]},\n",
    "    \"Neural Net\": {'alpha': [0.0001, 0.001, 0.01]},\n",
    "    \"AdaBoost\": {'n_estimators': [50, 100, 200]},\n",
    "    \"Naive Bayes\": {},\n",
    "    \"SVM Linear\": {'C': [0.1, 1, 10]},\n",
    "    \"SVM RBF\": {'C': [0.1, 1, 10], 'gamma': [0.001, 0.01, 0.1]},\n",
    "    \"MultinomialNB\": {'alpha': [0.01, 0.001, 0.1, 1, 10]},\n",
    "    \"XGBClassifier\": {'n_estimators':[2, 3, 5, 10, 20, 100, 200], 'max_depth':[2, 3, 5, 7]} \n",
    "}\n",
    "\n",
    "# Models\n",
    "names = [\"Nearest Neighbors\", \"XGBClassifier\", \"Gaussian Process\", \"Random Forest\", \"Neural Net\", \"AdaBoost\", \"Naive Bayes\", \"SVM Linear\", \"SVM RBF\"]\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    XGBClassifier(objective='binary:logistic'),\n",
    "    GaussianProcessClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    MLPClassifier(max_iter=10000, early_stopping=False),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    SVC(kernel='linear'),\n",
    "    SVC(kernel='rbf')\n",
    "]\n",
    "models = zip(names, classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3058456-c70c-4172-a3cb-df13a6eef0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Nearest Neighbors...\n",
      "Best params found:  {'n_neighbors': 5}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'Nearest Neighbors' - acc: 0.7863440860215053 ±(0.08437270656518296)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'Nearest Neighbors' - acc : 0.8026315789473685\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.94      0.80        32\n",
      "           1       0.94      0.70      0.81        44\n",
      "\n",
      "    accuracy                           0.80        76\n",
      "   macro avg       0.82      0.82      0.80        76\n",
      "weighted avg       0.84      0.80      0.80        76\n",
      "\n",
      "Processing XGBClassifier...\n",
      "Best params found:  {'max_depth': 2, 'n_estimators': 100}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'XGBClassifier' - acc: 0.881505376344086 ±(0.053432962788091426)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'XGBClassifier' - acc : 0.9078947368421053\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.84      0.89        32\n",
      "           1       0.89      0.95      0.92        44\n",
      "\n",
      "    accuracy                           0.91        76\n",
      "   macro avg       0.91      0.90      0.90        76\n",
      "weighted avg       0.91      0.91      0.91        76\n",
      "\n",
      "Processing Gaussian Process...\n",
      "Best params found:  {'kernel': 1**2 * RBF(length_scale=1)}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'Gaussian Process' - acc: 0.9015053763440861 ±(0.06179980658825857)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'Gaussian Process' - acc : 0.9342105263157895\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93        32\n",
      "           1       0.98      0.91      0.94        44\n",
      "\n",
      "    accuracy                           0.93        76\n",
      "   macro avg       0.93      0.94      0.93        76\n",
      "weighted avg       0.94      0.93      0.93        76\n",
      "\n",
      "Processing Random Forest...\n",
      "Best params found:  {'max_depth': 7, 'max_features': 3, 'n_estimators': 100}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'Random Forest' - acc: 0.7895698924731184 ±(0.08050748104003005)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'Random Forest' - acc : 0.8421052631578947\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.66      0.78        32\n",
      "           1       0.80      0.98      0.88        44\n",
      "\n",
      "    accuracy                           0.84        76\n",
      "   macro avg       0.88      0.82      0.83        76\n",
      "weighted avg       0.86      0.84      0.84        76\n",
      "\n",
      "Processing Neural Net...\n",
      "Best params found:  {'alpha': 0.0001}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'Neural Net' - acc: 0.9112903225806452 ±(0.05451570486144924)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'Neural Net' - acc : 0.9605263157894737\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95        32\n",
      "           1       0.98      0.95      0.97        44\n",
      "\n",
      "    accuracy                           0.96        76\n",
      "   macro avg       0.96      0.96      0.96        76\n",
      "weighted avg       0.96      0.96      0.96        76\n",
      "\n",
      "Processing AdaBoost...\n",
      "Best params found:  {'n_estimators': 200}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'AdaBoost' - acc: 0.9110752688172044 ±(0.04139170461307206)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'AdaBoost' - acc : 0.9078947368421053\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.94      0.90        32\n",
      "           1       0.95      0.89      0.92        44\n",
      "\n",
      "    accuracy                           0.91        76\n",
      "   macro avg       0.90      0.91      0.91        76\n",
      "weighted avg       0.91      0.91      0.91        76\n",
      "\n",
      "Processing Naive Bayes...\n",
      "Best params found:  {}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'Naive Bayes' - acc: 0.8651612903225807 ±(0.05696936151328498)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'Naive Bayes' - acc : 0.881578947368421\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.94      0.87        32\n",
      "           1       0.95      0.84      0.89        44\n",
      "\n",
      "    accuracy                           0.88        76\n",
      "   macro avg       0.88      0.89      0.88        76\n",
      "weighted avg       0.89      0.88      0.88        76\n",
      "\n",
      "Processing SVM Linear...\n",
      "Best params found:  {'C': 0.1}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'SVM Linear' - acc: 0.5717204301075268 ±(0.0953552485428745)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'SVM Linear' - acc : 0.5789473684210527\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        32\n",
      "           1       0.58      1.00      0.73        44\n",
      "\n",
      "    accuracy                           0.58        76\n",
      "   macro avg       0.29      0.50      0.37        76\n",
      "weighted avg       0.34      0.58      0.42        76\n",
      "\n",
      "Processing SVM RBF...\n",
      "Best params found:  {'C': 0.1, 'gamma': 0.001}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'SVM RBF' - acc: 0.5717204301075268 ±(0.0953552485428745)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'SVM RBF' - acc : 0.5789473684210527\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        32\n",
      "           1       0.58      1.00      0.73        44\n",
      "\n",
      "    accuracy                           0.58        76\n",
      "   macro avg       0.29      0.50      0.37        76\n",
      "weighted avg       0.34      0.58      0.42        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "names   = []\n",
    "best_parameters = []\n",
    "\n",
    "for name, model in models:\n",
    "    print(f\"Processing {name}...\")\n",
    "    param_grid = param_grids[name]\n",
    "    kfold = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "    \n",
    "    # Perform grid search & Get the best model\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=kfold, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_5_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    best_parameters.append((name, best_params))\n",
    "    print('Best params found: ', best_params)\n",
    "    \n",
    "    # Cross-validation results\n",
    "    cv_results = cross_val_score(best_model, X_5_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"[Train] - '{}' - acc: {} ±({})\".format(name, cv_results.mean(), cv_results.std())\n",
    "    print(\"{}\\n{}\\n{}\".format('-'*80, msg, '-'*80))\n",
    "    \n",
    "    # Fit the best model\n",
    "    best_model.fit(X_5_train, y_train)\n",
    "    \n",
    "    # Make predictions and print test results\n",
    "    predictions = best_model.predict(X_5_test)\n",
    "    print(\"[Test]  - '{}' - acc : {}\".format(name, accuracy_score(y_test, predictions)))\n",
    "    print(classification_report(y_test, predictions), end=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc43cd06-ef86-4141-baea-85d1d0813b5e",
   "metadata": {},
   "source": [
    "<h4 style=\"background-color: #80c4e6; display: flex;\">\n",
    "    <ul><li>k=6</li></ul>\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d52deffc-7326-445d-8cb2-363d19b71e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 6\n",
    "sequences   = dataset['sequence']\n",
    "kmers_count = []\n",
    "for i in range(len(sequences)):\n",
    "    kmers_count.append(kmer_count(sequences[i], k=k, step=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04dd283a-dca3-40e6-a73e-9570429b0e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAAAAA</th>\n",
       "      <th>AAAAAC</th>\n",
       "      <th>AAAAAG</th>\n",
       "      <th>AAAAAT</th>\n",
       "      <th>AAAACA</th>\n",
       "      <th>AAAACC</th>\n",
       "      <th>AAAACG</th>\n",
       "      <th>AAAACT</th>\n",
       "      <th>AAAAGA</th>\n",
       "      <th>AAAAGC</th>\n",
       "      <th>...</th>\n",
       "      <th>TTTTCG</th>\n",
       "      <th>TTTTCT</th>\n",
       "      <th>TTTTGA</th>\n",
       "      <th>TTTTGC</th>\n",
       "      <th>TTTTGG</th>\n",
       "      <th>TTTTGT</th>\n",
       "      <th>TTTTTA</th>\n",
       "      <th>TTTTTC</th>\n",
       "      <th>TTTTTG</th>\n",
       "      <th>TTTTTT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.002182</td>\n",
       "      <td>0.001309</td>\n",
       "      <td>0.001309</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>0.001745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.002725</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.002044</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.002044</td>\n",
       "      <td>0.004768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.002032</td>\n",
       "      <td>0.002032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.001364</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.002727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.002727</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.004091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002509</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>0.003512</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.006021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4096 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AAAAAA    AAAAAC    AAAAAG    AAAAAT    AAAACA    AAAACC    AAAACG  \\\n",
       "0  0.002182  0.000000  0.000436  0.000436  0.000436  0.000436  0.000000   \n",
       "1  0.005450  0.000000  0.000681  0.002725  0.000681  0.000000  0.000000   \n",
       "2  0.000339  0.000339  0.000339  0.001016  0.000677  0.000000  0.000339   \n",
       "3  0.005000  0.001364  0.001818  0.000455  0.000455  0.000455  0.000000   \n",
       "4  0.001004  0.001505  0.001505  0.000502  0.002007  0.000000  0.000502   \n",
       "\n",
       "     AAAACT    AAAAGA    AAAAGC  ...    TTTTCG    TTTTCT    TTTTGA    TTTTGC  \\\n",
       "0  0.000000  0.000436  0.000873  ...  0.000436  0.001745  0.000873  0.000873   \n",
       "1  0.000681  0.000681  0.000000  ...  0.000000  0.004087  0.000000  0.001362   \n",
       "2  0.000339  0.001355  0.000339  ...  0.000339  0.002370  0.001693  0.000339   \n",
       "3  0.000455  0.001818  0.000455  ...  0.000455  0.002727  0.000000  0.000909   \n",
       "4  0.000502  0.000000  0.001004  ...  0.000000  0.002509  0.000502  0.000000   \n",
       "\n",
       "     TTTTGG    TTTTGT    TTTTTA    TTTTTC    TTTTTG    TTTTTT  \n",
       "0  0.002182  0.001309  0.001309  0.000873  0.001745  0.001745  \n",
       "1  0.000681  0.001362  0.002044  0.001362  0.002044  0.004768  \n",
       "2  0.000339  0.002370  0.001355  0.001016  0.002032  0.002032  \n",
       "3  0.000455  0.000909  0.002727  0.001818  0.000455  0.004091  \n",
       "4  0.001505  0.002007  0.003512  0.001004  0.000502  0.006021  \n",
       "\n",
       "[5 rows x 4096 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = DictVectorizer(sparse=False)\n",
    "feature_values = v.fit_transform(kmers_count)\n",
    "feature_names = v.get_feature_names_out()\n",
    "X_6 = pd.DataFrame(feature_values, columns=feature_names)\n",
    "X_6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bdedd32-e7e4-4ebb-b7e0-fcdb0fd8a847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of train/test splits:\n",
      "X_train: (304, 4096)\n",
      "X_test: (76, 4096)\n",
      "y_train: (304,)\n",
      "y_test: (76,)\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "X_6_train, X_6_test, y_train, y_test = train_test_split(X_6, y, train_size=0.8, stratify=y)\n",
    "\n",
    "print(\"Shapes of train/test splits:\")\n",
    "print(\"X_train:\", X_6_train.shape)\n",
    "print(\"X_test:\", X_6_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00678a3c-16b7-4f75-81ec-df9e624ecd30",
   "metadata": {},
   "source": [
    "* Let tune our differents model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec50082e-97b6-4a18-ab5b-f9287b6c78d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grids\n",
    "param_grids = {\n",
    "    \"Nearest Neighbors\": {'n_neighbors': [3, 5, 7]},\n",
    "    \"Gaussian Process\": {'kernel': [1.0 * RBF(1.0), 1.0 * RBF(0.5), 1.0 * RBF(2.0)]},\n",
    "    \"Random Forest\": {'max_depth': [3, 5, 7], 'n_estimators': [10, 50, 100], 'max_features': [1, 2, 3]},\n",
    "    \"Neural Net\": {'alpha': [0.0001, 0.001, 0.01]},\n",
    "    \"AdaBoost\": {'n_estimators': [50, 100, 200]},\n",
    "    \"Naive Bayes\": {},\n",
    "    \"SVM Linear\": {'C': [0.1, 1, 10]},\n",
    "    \"SVM RBF\": {'C': [0.1, 1, 10], 'gamma': [0.001, 0.01, 0.1]},\n",
    "    \"XGBClassifier\": {'n_estimators':[2, 3, 5, 10, 20, 100, 200], 'max_depth':[2, 3, 5, 7]}  \n",
    "}\n",
    "\n",
    "# Models\n",
    "names = [\"Nearest Neighbors\", \"XGBClassifier\", \"Gaussian Process\", \"Random Forest\", \"Neural Net\", \"AdaBoost\", \"Naive Bayes\", \"SVM Linear\", \"SVM RBF\"]\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    XGBClassifier(objective='binary:logistic'),\n",
    "    GaussianProcessClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    MLPClassifier(max_iter=10000, early_stopping=False),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    SVC(kernel='linear'),\n",
    "    SVC(kernel='rbf')\n",
    "]\n",
    "models = zip(names, classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6c7421d-88bb-4680-b8d9-68f82f46e3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Nearest Neighbors...\n",
      "Best params found:  {'n_neighbors': 5}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'Nearest Neighbors' - acc: 0.7662365591397851 ±(0.054324385500287224)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'Nearest Neighbors' - acc : 0.7894736842105263\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.66      0.72        32\n",
      "           1       0.78      0.89      0.83        44\n",
      "\n",
      "    accuracy                           0.79        76\n",
      "   macro avg       0.79      0.77      0.78        76\n",
      "weighted avg       0.79      0.79      0.79        76\n",
      "\n",
      "Processing XGBClassifier...\n",
      "Best params found:  {'max_depth': 2, 'n_estimators': 200}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'XGBClassifier' - acc: 0.8981720430107527 ±(0.0516376508769629)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'XGBClassifier' - acc : 0.8026315789473685\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.66      0.74        32\n",
      "           1       0.78      0.91      0.84        44\n",
      "\n",
      "    accuracy                           0.80        76\n",
      "   macro avg       0.81      0.78      0.79        76\n",
      "weighted avg       0.81      0.80      0.80        76\n",
      "\n",
      "Processing Gaussian Process...\n",
      "Best params found:  {'kernel': 1**2 * RBF(length_scale=0.5)}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'Gaussian Process' - acc: 0.9112903225806452 ±(0.04185061718615743)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'Gaussian Process' - acc : 0.8947368421052632\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.84      0.87        32\n",
      "           1       0.89      0.93      0.91        44\n",
      "\n",
      "    accuracy                           0.89        76\n",
      "   macro avg       0.90      0.89      0.89        76\n",
      "weighted avg       0.89      0.89      0.89        76\n",
      "\n",
      "Processing Random Forest...\n",
      "Best params found:  {'max_depth': 7, 'max_features': 2, 'n_estimators': 50}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'Random Forest' - acc: 0.7243010752688173 ±(0.06582577550280537)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'Random Forest' - acc : 0.6973684210526315\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.38      0.51        32\n",
      "           1       0.67      0.93      0.78        44\n",
      "\n",
      "    accuracy                           0.70        76\n",
      "   macro avg       0.74      0.65      0.65        76\n",
      "weighted avg       0.73      0.70      0.67        76\n",
      "\n",
      "Processing Neural Net...\n",
      "Best params found:  {'alpha': 0.0001}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'Neural Net' - acc: 0.9308602150537635 ±(0.037763373502768934)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'Neural Net' - acc : 0.9078947368421053\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89        32\n",
      "           1       0.91      0.93      0.92        44\n",
      "\n",
      "    accuracy                           0.91        76\n",
      "   macro avg       0.91      0.90      0.91        76\n",
      "weighted avg       0.91      0.91      0.91        76\n",
      "\n",
      "Processing AdaBoost...\n",
      "Best params found:  {'n_estimators': 200}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'AdaBoost' - acc: 0.894731182795699 ±(0.051000964626934146)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'AdaBoost' - acc : 0.8552631578947368\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.75      0.81        32\n",
      "           1       0.84      0.93      0.88        44\n",
      "\n",
      "    accuracy                           0.86        76\n",
      "   macro avg       0.86      0.84      0.85        76\n",
      "weighted avg       0.86      0.86      0.85        76\n",
      "\n",
      "Processing Naive Bayes...\n",
      "Best params found:  {}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'Naive Bayes' - acc: 0.8289247311827956 ±(0.06001011592374)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'Naive Bayes' - acc : 0.868421052631579\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84        32\n",
      "           1       0.89      0.89      0.89        44\n",
      "\n",
      "    accuracy                           0.87        76\n",
      "   macro avg       0.87      0.87      0.87        76\n",
      "weighted avg       0.87      0.87      0.87        76\n",
      "\n",
      "Processing SVM Linear...\n",
      "Best params found:  {'C': 0.1}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'SVM Linear' - acc: 0.5731182795698925 ±(0.06866919567390532)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'SVM Linear' - acc : 0.5789473684210527\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        32\n",
      "           1       0.58      1.00      0.73        44\n",
      "\n",
      "    accuracy                           0.58        76\n",
      "   macro avg       0.29      0.50      0.37        76\n",
      "weighted avg       0.34      0.58      0.42        76\n",
      "\n",
      "Processing SVM RBF...\n",
      "Best params found:  {'C': 0.1, 'gamma': 0.001}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'SVM RBF' - acc: 0.5731182795698925 ±(0.06866919567390532)\n",
      "--------------------------------------------------------------------------------\n",
      "[Test]  - 'SVM RBF' - acc : 0.5789473684210527\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        32\n",
      "           1       0.58      1.00      0.73        44\n",
      "\n",
      "    accuracy                           0.58        76\n",
      "   macro avg       0.29      0.50      0.37        76\n",
      "weighted avg       0.34      0.58      0.42        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "names   = []\n",
    "best_parameters = []\n",
    "\n",
    "for name, model in models:\n",
    "    print(f\"Processing {name}...\")\n",
    "    param_grid = param_grids[name]\n",
    "    kfold = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "    \n",
    "    # Perform grid search & Get the best model\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=kfold, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_6_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    best_parameters.append((name, best_params))\n",
    "    print('Best params found: ', best_params)\n",
    "    \n",
    "    # Cross-validation results\n",
    "    cv_results = cross_val_score(best_model, X_6_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"[Train] - '{}' - acc: {} ±({})\".format(name, cv_results.mean(), cv_results.std())\n",
    "    print(\"{}\\n{}\\n{}\".format('-'*80, msg, '-'*80))\n",
    "    \n",
    "    # Fit the best model\n",
    "    best_model.fit(X_6_train, y_train)\n",
    "    \n",
    "    # Make predictions and print test results\n",
    "    predictions = best_model.predict(X_6_test)\n",
    "    print(\"[Test]  - '{}' - acc : {}\".format(name, accuracy_score(y_test, predictions)))\n",
    "    print(classification_report(y_test, predictions), end=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecef1a6-9b96-4df2-ba72-80dea3e43758",
   "metadata": {},
   "source": [
    "<h4 style=\"background-color: #80c4e6; border-top: 4px solid #dddddd; display: flex; color: white;\">\n",
    "    <ul><li>Testing various levels of granularity: k=3 & k=6</li></ul>\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b2b2c1-ebbb-47ea-a3d9-2587eb089ec7",
   "metadata": {},
   "source": [
    "* I Want to test this feature selection methode to take only relevant feature for this k-mer frequence approach\n",
    "* https://link-springer-com.eressources.um6p.ma/chapter/10.1007/978-3-319-24462-4_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2c5ce700-cf9f-4c24-9114-4289cc6cf469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAA</th>\n",
       "      <th>AAC</th>\n",
       "      <th>AAG</th>\n",
       "      <th>AAT</th>\n",
       "      <th>ACA</th>\n",
       "      <th>ACC</th>\n",
       "      <th>ACG</th>\n",
       "      <th>ACT</th>\n",
       "      <th>AGA</th>\n",
       "      <th>AGC</th>\n",
       "      <th>...</th>\n",
       "      <th>TTTTCG</th>\n",
       "      <th>TTTTCT</th>\n",
       "      <th>TTTTGA</th>\n",
       "      <th>TTTTGC</th>\n",
       "      <th>TTTTGG</th>\n",
       "      <th>TTTTGT</th>\n",
       "      <th>TTTTTA</th>\n",
       "      <th>TTTTTC</th>\n",
       "      <th>TTTTTG</th>\n",
       "      <th>TTTTTT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>0.023613</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.022432</td>\n",
       "      <td>0.018890</td>\n",
       "      <td>0.011806</td>\n",
       "      <td>0.007084</td>\n",
       "      <td>0.014168</td>\n",
       "      <td>0.011806</td>\n",
       "      <td>0.028926</td>\n",
       "      <td>0.012397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.035915</td>\n",
       "      <td>0.016835</td>\n",
       "      <td>0.029742</td>\n",
       "      <td>0.027497</td>\n",
       "      <td>0.013468</td>\n",
       "      <td>0.007295</td>\n",
       "      <td>0.008979</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.035915</td>\n",
       "      <td>0.010662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.001124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0.033218</td>\n",
       "      <td>0.022837</td>\n",
       "      <td>0.022837</td>\n",
       "      <td>0.015917</td>\n",
       "      <td>0.021453</td>\n",
       "      <td>0.017993</td>\n",
       "      <td>0.005536</td>\n",
       "      <td>0.017301</td>\n",
       "      <td>0.032526</td>\n",
       "      <td>0.013149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.026626</td>\n",
       "      <td>0.015277</td>\n",
       "      <td>0.022698</td>\n",
       "      <td>0.024443</td>\n",
       "      <td>0.016587</td>\n",
       "      <td>0.013531</td>\n",
       "      <td>0.019206</td>\n",
       "      <td>0.010039</td>\n",
       "      <td>0.021388</td>\n",
       "      <td>0.011785</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.002185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.030612</td>\n",
       "      <td>0.022809</td>\n",
       "      <td>0.030612</td>\n",
       "      <td>0.024010</td>\n",
       "      <td>0.022809</td>\n",
       "      <td>0.009604</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>0.015006</td>\n",
       "      <td>0.025810</td>\n",
       "      <td>0.012605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.001804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4160 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          AAA       AAC       AAG       AAT       ACA       ACC       ACG  \\\n",
       "292  0.023613  0.018300  0.022432  0.018890  0.011806  0.007084  0.014168   \n",
       "122  0.035915  0.016835  0.029742  0.027497  0.013468  0.007295  0.008979   \n",
       "277  0.033218  0.022837  0.022837  0.015917  0.021453  0.017993  0.005536   \n",
       "191  0.026626  0.015277  0.022698  0.024443  0.016587  0.013531  0.019206   \n",
       "246  0.030612  0.022809  0.030612  0.024010  0.022809  0.009604  0.008403   \n",
       "\n",
       "          ACT       AGA       AGC  ...    TTTTCG    TTTTCT    TTTTGA  \\\n",
       "292  0.011806  0.028926  0.012397  ...  0.000591  0.001183  0.000000   \n",
       "122  0.015713  0.035915  0.010662  ...  0.000000  0.000562  0.001124   \n",
       "277  0.017301  0.032526  0.013149  ...  0.000000  0.002080  0.000000   \n",
       "191  0.010039  0.021388  0.011785  ...  0.000000  0.002185  0.000000   \n",
       "246  0.015006  0.025810  0.012605  ...  0.000000  0.001203  0.001804   \n",
       "\n",
       "       TTTTGC    TTTTGG    TTTTGT    TTTTTA    TTTTTC    TTTTTG    TTTTTT  \n",
       "292  0.000000  0.000591  0.000000  0.000591  0.000591  0.000000  0.000000  \n",
       "122  0.001686  0.000000  0.000562  0.001124  0.000000  0.001124  0.001124  \n",
       "277  0.000000  0.000000  0.000000  0.000693  0.000000  0.000000  0.000000  \n",
       "191  0.000437  0.000000  0.000437  0.000437  0.001311  0.000874  0.002185  \n",
       "246  0.000000  0.000000  0.001203  0.000000  0.000601  0.000601  0.000000  \n",
       "\n",
       "[5 rows x 4160 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let combine X_3 and X_6 & Split data\n",
    "\n",
    "X_comb_36 = pd.concat([X_3, X_6], axis=1)\n",
    "X_36_train, X_36_test, y_train, y_test = train_test_split(X_comb_36, y, train_size=0.8, stratify=y)\n",
    "X_36_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dcd19aa8-1469-4a1b-a9bb-86a0670a5ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the feature names are consistent between train and test sets\n",
    "X_36_test = X_36_test.reindex(columns=X_36_train.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "31902668-cec1-4a64-9bff-82cd380b74c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grids\n",
    "param_grids = {\n",
    "    \"Gaussian Process\": {'kernel': [1.0 * RBF(1.0), 1.0 * RBF(0.5), 1.0 * RBF(2.0)]},\n",
    "}\n",
    "\n",
    "# Models\n",
    "names = [\"Gaussian Process\"]\n",
    "classifiers = [\n",
    "    GaussianProcessClassifier(),\n",
    "]\n",
    "models = zip(names, classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fe06e2a8-2da8-4071-ac84-cbe69ed083cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Gaussian Process...\n",
      "Best params found:  {'kernel': 1**2 * RBF(length_scale=1)}\n",
      "--------------------------------------------------------------------------------\n",
      "[Train] - 'Gaussian Process' - acc: 0.8884946236559139 ±(0.05022564415965383)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- AAA\n- AAC\n- AAG\n- AAT\n- ACA\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m best_model\u001b[38;5;241m.\u001b[39mfit(X_6_train, y_train)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Make predictions and print test results\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m predictions \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(X_36_test)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Test]  - \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m - acc : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, accuracy_score(y_test, predictions)))\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, predictions), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/gaussian_process/_gpc.py:774\u001b[0m, in \u001b[0;36mGaussianProcessClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    771\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mrequires_vector_input:\n\u001b[0;32m--> 774\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m\"\u001b[39m, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    776\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:548\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    485\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[1;32m    490\u001b[0m ):\n\u001b[1;32m    491\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \n\u001b[1;32m    493\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_names(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    552\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    554\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:481\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m    477\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    479\u001b[0m     )\n\u001b[0;32m--> 481\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- AAA\n- AAC\n- AAG\n- AAT\n- ACA\n- ...\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "names   = []\n",
    "best_parameters = []\n",
    "\n",
    "for name, model in models:\n",
    "    print(f\"Processing {name}...\")\n",
    "    param_grid = param_grids[name]\n",
    "    kfold = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "    \n",
    "    # Perform grid search & Get the best model\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=kfold, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_36_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    best_parameters.append((name, best_params))\n",
    "    print('Best params found: ', best_params)\n",
    "    \n",
    "    # Cross-validation results\n",
    "    cv_results = cross_val_score(best_model, X_36_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"[Train] - '{}' - acc: {} ±({})\".format(name, cv_results.mean(), cv_results.std())\n",
    "    print(\"{}\\n{}\\n{}\".format('-'*80, msg, '-'*80))\n",
    "    \n",
    "    # Fit the best model\n",
    "    best_model.fit(X_6_train, y_train)\n",
    "    \n",
    "    # Make predictions and print test results\n",
    "    predictions = best_model.predict(X_36_test)\n",
    "    print(\"[Test]  - '{}' - acc : {}\".format(name, accuracy_score(y_test, predictions)))\n",
    "    print(classification_report(y_test, predictions), end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27128d8c-6dee-4282-a0cc-d592fa2adc9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
