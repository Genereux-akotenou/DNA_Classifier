{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9f20e8d-1b39-46c6-8a5d-a14836a98ba4",
   "metadata": {},
   "source": [
    "**Features selection**: From high dimensionnal data to small one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d35be60-6a63-4624-8ac3-30b21c5b575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Feature processing\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import scale\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "# Model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Metric and utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import itertools\n",
    "\n",
    "import sys\n",
    "import os\n",
    "current_directory = os.getcwd()\n",
    "root_directory = os.path.abspath(os.path.join(current_directory, os.pardir))\n",
    "utils_directory = os.path.join(root_directory, 'Utils')\n",
    "sys.path.append(utils_directory)\n",
    "\n",
    "from DNA_Utils import DNA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6c66f4-d409-48a6-9755-120fe2406246",
   "metadata": {},
   "source": [
    "* Load test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b3a05a-11e3-43cc-b882-6ff979bd7682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = pd.read_csv(\"../Output/Arabidopsis_thaliana_GHLH_and_CYP_gene.csv\")\n",
    "dataset_ = pd.read_csv(\"../Output/kiwi_orange_cds.csv\")\n",
    "\n",
    "# Split\n",
    "dataset, testset = train_test_split(dataset_, test_size=0.2, stratify=dataset_[\"class\"], random_state=42)\n",
    "dataset = dataset.reset_index(drop=True)\n",
    "testset = testset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c29150-f65c-417a-9f01-def83b612516",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "X, y = DNA.build_kmer_representation(dataset, k=k)\n",
    "X_test_glob, y_test_glob = DNA.build_kmer_representation(testset, k=k)\n",
    "X_test_glob.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c752ebd-a822-450b-b489-98f326a6094b",
   "metadata": {},
   "source": [
    "* Base line for model selection: <span style=\"color: blue;\">Default Feature Importance: Random forests provide a built-in way to evaluate feature importance, which is helpful for feature selection tasks.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4436090-23c1-4618-a688-e399573ed034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned RF (pre-computed to save time)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_features': [2, 4, 8, 16],\n",
    "    'max_depth': [8, 16, 32, 64, 128]\n",
    "}\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X, y)\n",
    "best_rf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf22256f-c48b-4483-a9ee-e9448360ebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f290f8d-4114-4a19-97d9-801d258be3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = grid_search.best_score_\n",
    "best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ed632e-7e2a-4b74-a733-3541f8184c16",
   "metadata": {},
   "source": [
    "**Unsupervised feature selection**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484dc697-b752-4a37-b5c8-f868c80b52f0",
   "metadata": {},
   "source": [
    " * <span style=\"color: red\">Variance-based(1)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a307f3df-9828-4453-a6e2-eb73247fda23",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = VarianceThreshold().fit(MinMaxScaler().fit_transform(X))\n",
    "variances = selector.variances_\n",
    "\n",
    "plt.hist(variances, bins=50)\n",
    "plt.xlabel('Variance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Feature Variances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41258a6-a076-403c-92fc-53408025be59",
   "metadata": {},
   "source": [
    "Default Feature Importance: Random forests provide a built-in way to evaluate feature importance, which is helpful for feature selection tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e5d0b9-8589-41c2-855f-11a85ea326e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, variances.max(), 50)\n",
    "param_grid = {'variance_threshold__threshold': thresholds}\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('variance_threshold', VarianceThreshold()),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X, y)\n",
    "best_threshold = grid_search.best_params_['variance_threshold__threshold']\n",
    "print(\"Best threshold based on cross-validation:\", best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6016723d-8be5-41f6-a4b3-901fba2838b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = VarianceThreshold(threshold=best_threshold).fit(MinMaxScaler().fit_transform(X))\n",
    "variances = selector.variances_\n",
    "var_sort = np.argsort(variances)\n",
    "\n",
    "fig_scale = 3\n",
    "feature_count = 20\n",
    "plt.figure(figsize=(4*fig_scale, 1.5*fig_scale))\n",
    "ypos = np.arange(feature_count)[::-1]\n",
    "plt.barh(ypos, variances[var_sort][:feature_count], align='center')\n",
    "plt.yticks(ypos, np.array(X.columns.values)[var_sort][:feature_count])\n",
    "plt.xlabel(\"Variance\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15026130-bbee-4bc7-b4cb-328f9b095e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = X.columns[selector.get_support()]\n",
    "#print(\"Selected features based on initial quantile threshold:\", selected_features.tolist())\n",
    "print(f\"Before selection= {len(X.columns.values)} / After = {len(selected_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6c20db-258b-43c7-81ca-366ed44ce82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = grid_search.best_score_\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8827cbb0-ef8c-422c-8a9a-a60c48f5b64e",
   "metadata": {},
   "source": [
    "**Note This method is not revelant:** since the base line model with all feature give \"acc:0.8842\" and the optimized one give \"acc:0.894\" with just one feature reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0687fd26-ed4c-4bc3-a86b-d50b608ce23f",
   "metadata": {},
   "source": [
    "* <span style=\"color: red\">Covariance based feature selection (2)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dba67f-08d2-4eca-a7c1-a0d59736f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@features_names = X.columns.values\n",
    "#X_scaled = scale(X)\n",
    "#cov = np.cov(X_scaled, rowvar=False)\n",
    "\n",
    "#order = np.array(hierarchy.dendrogram(hierarchy.ward(cov), no_plot=True)['ivl'], dtype=\"int\")\n",
    "#features_names_ordered = [features_names[i] for i in order]\n",
    "\n",
    "#fig_scale=5\n",
    "#plt.figure(figsize=(3*fig_scale, 3*fig_scale))\n",
    "#plt.imshow(cov[order, :][:, order], cmap='bwr')\n",
    "#plt.xticks(range(X.shape[1]), features_names_ordered, ha=\"right\")\n",
    "#plt.yticks(range(X.shape[1]), features_names_ordered)\n",
    "#plt.xticks(rotation=45)\n",
    "#plt.colorbar(fraction=0.046, pad=0.04);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcba4f7f-f721-481d-92cd-ef0e13ba9d22",
   "metadata": {},
   "source": [
    "**Note**: It will be very difficult to select features here. Also, we should check whenever one of two features correlates more with the target. While correlation-based feature selection can be useful, it may not always be the best method for k-mer representations of DNA sequences due to the following reasons: k-mer features, especially for higher values of k, can result in very high-dimensional feature spaces, making correlation-based methods less effective; the relationships between k-mers and the target may be more complex and not adequately captured by simple linear correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1f2fad-4aaf-47e1-aaee-397f6e3a548f",
   "metadata": {},
   "source": [
    "* <span style=\"color: red\">Supervised feature selection (3)</span>\n",
    "\n",
    "    * Model-based\n",
    "    * Univariate feature selection is a type of feature selection technique used in machine learning to select the most relevant features from the original feature set. It operates by evaluating the relationship between each feature and the target variable independently, without considering the relationship between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caf4f31-e3a3-470f-9cfb-5d5b2d991679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils to plot feature importance \n",
    "\n",
    "def plot_feature_importances(method1='f_test', method2=None, threshold=0.5):\n",
    "    # Plot scores\n",
    "    features_names = X.columns.values\n",
    "    x = np.arange(len(X.columns.values))\n",
    "    fig_scale = 4\n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=(4*fig_scale, 1*fig_scale))\n",
    "    w = 0.3\n",
    "    imp = fs[method1]\n",
    "    mask = imp['select'][threshold]\n",
    "    m1 = ax1.bar(x[mask], imp['scaled_score'][mask], width=w, color='b', align='center')\n",
    "    ax1.bar(x[~mask], imp['scaled_score'][~mask], width=w, color='b', align='center', alpha=0.3)\n",
    "    if method2:\n",
    "        imp2 = fs[method2]\n",
    "        mask2 = imp2['select'][threshold]\n",
    "        ax2 = ax1.twinx()\n",
    "        m2 = ax2.bar(x[mask2] + w, imp2['scaled_score'][mask2], width=w,color='g',align='center')\n",
    "        ax2.bar(x[~mask2] + w, imp2['scaled_score'][~mask2], width=w,color='g',align='center', alpha=0.3)\n",
    "        plt.legend([m1, m2],['{} (Ridge R2:{:.2f})'.format(imp['label'],imp['cv_score'][threshold]),\n",
    "                             '{} (Ridge R2:{:.2f})'.format(imp2['label'],imp2['cv_score'][threshold])], loc='upper left')\n",
    "    else:\n",
    "        plt.legend([m1],['{} (Ridge R2:{:.2f})'.format(imp['label'],imp['cv_score'][threshold])], loc='upper left')\n",
    "    ax1.set_xticks(range(len(features_names)))\n",
    "    ax1.set_xticklabels(features_names, rotation=45, ha=\"right\");\n",
    "    plt.title(\"Feature importance (selection threshold {:.2f})\".format(threshold))        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c366ee-5a1a-4ccf-97e9-ec18a9fccede",
   "metadata": {},
   "source": [
    "**Note**: We will use different techniques(supervised learning) to do feature selection then we use out utils \"plot_feature_importances\" to visualize and analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8dbd7f-fee8-4952-bb69-57c2e68e20b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif, SelectPercentile, mutual_info_classif, SelectFromModel, RFE, SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Scaled feature selection thresholds\n",
    "thresholds = [0.25, 0.5, 0.75, 1]\n",
    "\n",
    "# Dict to store all data\n",
    "fs = {}\n",
    "methods = [\n",
    "    'FTest','MutualInformation','RandomForest','Ridge','Lasso','RFE', 'ForwardSelection','FloatingForwardSelection', 'Permutation', \n",
    "    'knn'\n",
    "]\n",
    "for m in methods:\n",
    "    fs[m] = {}\n",
    "    fs[m]['select'] = {}\n",
    "    fs[m]['cv_score'] = {}\n",
    "\n",
    "def cv_score(selector):\n",
    "    model = RandomForestClassifier()\n",
    "    select_pipe = make_pipeline(StandardScaler(), selector, model)    \n",
    "    return np.mean(cross_val_score(select_pipe, X, y, cv=5))\n",
    "\n",
    "# We already tuned RF (pre-computed to save time)\n",
    "randomforestCV = RandomForestClassifier(n_estimators=100, max_features=8, max_depth=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c544e2-0355-49e0-a898-696a9ead46ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a 'template' model with full features and another one that will use selected features to see if feature selection \n",
    "# improves the model.\n",
    "\n",
    "def assess_model_on_full_features(model):\n",
    "    kfold = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "    cv_scores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\n",
    "    print(\"[ALL_FEATURE] -- Cross-validated accuracy: {:.4f} ± {:.4f}\".format(np.mean(cv_scores), np.std(cv_scores)))\n",
    "\n",
    "def assess_model_on_selected_features(mask, model):\n",
    "    kfold = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "    mask_series = pd.Series(mask, index=X.columns)\n",
    "    index_where_true = mask_series.index[mask_series.values]\n",
    "    X_reduced = X[index_where_true] \n",
    "    cv_scores = cross_val_score(model, X_reduced, y, cv=kfold, scoring='accuracy')\n",
    "    print(\"[SELECTED_FEATURE] -- Cross-validated accuracy: {:.4f} ± {:.4f}\".format(np.mean(cv_scores), np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4840cd92-e52d-46b7-b64e-a530d9f24cbe",
   "metadata": {},
   "source": [
    "* F-TEST\n",
    "    * Consider each feature individually (univariate), independent of the model that you aim to apply\n",
    "    * Use a statistical test: is there a linear statistically significant relationship with the target?\n",
    "    * Use F-statistic (or corresponding p value) to rank all features, then select features using a threshold\n",
    "    * Cannot detect correlations or interactions (e.g. binary features)\n",
    "    * <span style=\"color: red;\">F-TEST</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c9c1b8-129c-4ee6-b26e-e698401aee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F test\n",
    "print(\"Computing F test\")\n",
    "fs['FTest']['label'] = \"F test\"\n",
    "fs['FTest']['score'] = f_classif(scale(X),y)[0]\n",
    "fs['FTest']['scaled_score'] = fs['FTest']['score'] / np.max(fs['FTest']['score'])\n",
    "for t in tqdm(thresholds):\n",
    "    selector = SelectPercentile(score_func=f_classif, percentile=t*100).fit(scale(X), y)\n",
    "    fs['FTest']['select'][t] = selector.get_support()\n",
    "    fs['FTest']['cv_score'][t] = cv_score(selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b61be9-977d-48cb-8fd9-4c6cff58612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.25\n",
    "plot_feature_importances('FTest', None, threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb9b79c-fbee-4020-98f8-0ca11ece4599",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, max_features=8, max_depth=16)\n",
    "assess_model_on_full_features(model)\n",
    "assess_model_on_selected_features(fs['FTest']['select'][threshold], model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03ec632-f3de-48d7-9723-0f63db1994b5",
   "metadata": {},
   "source": [
    "**Note**: For any value of threshold it does not improve prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e92b25c-b2b3-49a0-b718-aa19e895656d",
   "metadata": {},
   "source": [
    "* Model based feature selection\n",
    "    * Use a tuned(!) supervised model to judge the importance of each feature\n",
    "    * Linear models (Ridge, Lasso, LinearSVM,…): features with highest weights (coefficients)\n",
    "    * Tree–based models: features used in first nodes (high information gain)\n",
    "    * Captures interactions: features are more/less informative in combination\n",
    "    * RandomForests: learns complex interactions (e.g. hour), but biased to high cardinality features\n",
    "    * <span style=\"color: red\">Random Forest</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdbeedd-2ca8-4362-b5bb-104df17c6992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "print(\"Computing Random Forest\")\n",
    "rf = randomforestCV\n",
    "fs['RandomForest']['label'] = \"Random Forest\"\n",
    "fs['RandomForest']['score'] = rf.fit(X, y).feature_importances_\n",
    "fs['RandomForest']['scaled_score'] = fs['RandomForest']['score'] / np.max(fs['RandomForest']['score'])\n",
    "\n",
    "for t in tqdm(thresholds):\n",
    "    selector = SelectFromModel(rf, threshold=\"{}*mean\".format((1-t)*2)).fit(X, y)\n",
    "    fs['RandomForest']['select'][t] = selector.get_support()\n",
    "    fs['RandomForest']['cv_score'][t] = cv_score(selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1581a6-daf0-428d-b235-135ba5f3f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.25\n",
    "plot_feature_importances('RandomForest', None, threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c7f7d0-29d6-419d-9174-c18b0854524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, max_features=8, max_depth=16)\n",
    "assess_model_on_full_features(model)\n",
    "assess_model_on_selected_features(fs['RandomForest']['select'][threshold], model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dd82ae-cfad-4c21-9e2b-6471e2bac541",
   "metadata": {},
   "source": [
    "* Model based feature selection\n",
    "    * Use a tuned(!) supervised model to judge the importance of each feature\n",
    "    * Linear models (Ridge, Lasso, LinearSVM,…): features with highest weights (coefficients)\n",
    "    * Tree–based models: features used in first nodes (high information gain)\n",
    "    * Captures interactions: features are more/less informative in combination\n",
    "    * RandomForests: learns complex interactions (e.g. hour), but biased to high cardinality features\n",
    "    * * The LASSO method regularizes model parameters by shrinking the regression coefficients, reducing some of them to zero. The feature selection phase occurs after the shrinkage, where every non-zero value is selected to be used in the model. This method is significant in the minimization of prediction errors that are common in statistical models.\n",
    "    * * LASSO offers models with high prediction accuracy. The accuracy increases since the method includes shrinkage of coefficients, which reduces variance and minimizes bias. It performs best when the number of observations is low and the number of features is high. It heavily relies on parameter λ, which is the controlling factor in shrinkage. The larger λ becomes, then the more coefficients are forced to be zero.\n",
    "    * <span style=\"color: red\">Ridge, Lasso</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c259366d-5cb4-4ef9-8d93-5f5c0f8a24fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge, Lasso\n",
    "for m in [RidgeCV(),LassoCV()]:\n",
    "    name = m.__class__.__name__.replace('CV','')\n",
    "    print(\"Computing\", name)\n",
    "    fs[name]['label'] = name\n",
    "    fs[name]['score'] = m.fit(X, y).coef_\n",
    "    fs[name]['scaled_score'] = np.abs(fs[name]['score']) / np.max(np.abs(fs[name]['score'])) # Use absolute values\n",
    "    for t in tqdm(thresholds):\n",
    "        selector = SelectFromModel(m, threshold=\"{}*mean\".format((1-t)*2)).fit(scale(X), y)\n",
    "        fs[name]['select'][t] = selector.get_support()\n",
    "        fs[name]['cv_score'][t] = cv_score(selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ccee98-20ee-45d0-8092-9fd7cc847c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.25\n",
    "plot_feature_importances('Ridge', 'Lasso', threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477b176c-fcc4-48db-8161-505ba66c86be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, max_features=8, max_depth=16)\n",
    "assess_model_on_full_features(model)\n",
    "assess_model_on_selected_features(fs['Lasso']['select'][threshold], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb89555-d5f6-411d-93bf-fe68eb66ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutual information\n",
    "print(\"Computing Mutual information\")\n",
    "fs['MutualInformation']['label'] = \"Mutual Information\"\n",
    "fs['MutualInformation']['score'] = mutual_info_classif(scale(X),y,discrete_features=range(13)) # first 13 features are discrete\n",
    "fs['MutualInformation']['scaled_score'] = fs['MutualInformation']['score'] / np.max(fs['MutualInformation']['score'])\n",
    "for t in tqdm(thresholds):\n",
    "    selector = SelectPercentile(score_func=mutual_info_classif, percentile=t*100).fit(scale(X), y)\n",
    "    fs['MutualInformation']['select'][t] = selector.get_support()\n",
    "    fs['MutualInformation']['cv_score'][t] = cv_score(selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd390d54-8184-4f2f-872b-6c24fd9e3cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.25\n",
    "plot_feature_importances('MutualInformation', None, threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4200c786-0402-4298-94e2-30104bb76519",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, max_features=8, max_depth=16)\n",
    "assess_model_on_full_features(model)\n",
    "assess_model_on_selected_features(fs['MutualInformation']['select'][threshold], model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e0e84d-4d08-4a8a-9c87-86891ba82045",
   "metadata": {},
   "source": [
    "* <span style=\"color: red\">New Feature selection for k-mer (Motif Independent Measure (MIM))</span>\n",
    "    * https://www.researchgate.net/publication/268977663_A_new_feature_selection_strategy_for_K-mers_sequence_representation\n",
    "    * **Paper**: The idea behind the proposed feature selection method is to assign a weight to each k-mer, and use this weights for their selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56c8616-4b02-409b-88dc-f4d3ed13be94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let P be the k-mer probability distributions corresponding to a set of n target sequences S = {si}\n",
    "P = X.to_numpy()\n",
    "# Let Q be the k-mer probability distributions corresponding to B for a fixed length k.\n",
    "Q = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18015eb9-d584-4f14-ba67-cd8e54fcc1fc",
   "metadata": {},
   "source": [
    "Symmetrical Kullback-Leibler divergence between the empirical probabilities Pj and Qj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7de9884-45a4-4527-94b4-5a367a050ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kullback_leibler_divergence(P, Q):\n",
    "    J = P.shape[1]\n",
    "    dkl = np.zeros(J)\n",
    "    epsilon = 1e-10\n",
    "    for j in range(J):\n",
    "        P_j = P[:, j]\n",
    "        Q_j = Q[:, j]\n",
    "        P_j = np.where(P_j == 0, epsilon, P_j)\n",
    "        Q_j = np.where(Q_j == 0, epsilon, Q_j)\n",
    "        dkl[j] = 0.5 * (np.sum(P_j * np.log(P_j / Q_j)) + np.sum(Q_j * np.log(Q_j / P_j)))\n",
    "    return dkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7889ef-16be-44e7-ad61-c0f1f0a3bbf0",
   "metadata": {},
   "source": [
    "The Motif Independent Measure (MIM) value corresponding to ak-mer wj is defined as the expected value dkl(Pj , Qj ), which is estimated by averagingover a finite set N > n of background sequences, and is indicated as MIM(wj ). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18591642-f448-4277-9472-f337b975e346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MIM_Feature_Selector(P, Q, alpha, beta):\n",
    "    assert alpha < 1 and beta < 1, \"Error: alpha and beta should be lower than 1\"\n",
    "    m = P.shape[1]\n",
    "    mim_values = kullback_leibler_divergence(P, Q)\n",
    "    z_scores = zscore(mim_values)\n",
    "    \n",
    "    # Identify k-mers with Z-scores above the threshold\n",
    "    A_alpha_pos = np.where(np.abs(z_scores) > alpha)[0]\n",
    "    A_alpha = mim_values[A_alpha_pos]\n",
    "    \n",
    "    # Select the number of k-mers based on the criteria\n",
    "    r = int(max(np.linalg.norm(A_alpha, ord=1), int(beta * m)))\n",
    "    selected_kmers_indices = np.argsort(mim_values)[:r]\n",
    "\n",
    "    # Generate mask\n",
    "    selection_mask = np.zeros(m, dtype=bool)\n",
    "    selection_mask[selected_kmers_indices] = True\n",
    "    \n",
    "    return selection_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93938677-c3ae-4312-b851-a4145cf4b67f",
   "metadata": {},
   "source": [
    "* Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff43bf25-18ee-44b4-b727-e6bcb621572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING FULL FEATURE AS BACKGROUND SEQUENCE\n",
    "\n",
    "alpha = 0.7\n",
    "beta = 0.5\n",
    "feature_mask_1 = MIM_Feature_Selector(P, P, alpha, beta)\n",
    "print(f\"Feature before: {len(X.columns.values)} - Feature After: {np.sum(feature_mask_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed91b7-ead2-4c9f-a6c8-d1f86b3a6b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, max_features=8, max_depth=16) \n",
    "assess_model_on_full_features(model) \n",
    "assess_model_on_selected_features(feature_mask_1, model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbabc95-d152-44ae-8802-3905e3a1838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING NEGATIVE BACKGROUND AS BACKGROUND SEQUENCE\n",
    "kmers_count = []\n",
    "sequences = dataset['sequence']\n",
    "for i in range(len(sequences)):\n",
    "    kmers_count.append(DNA.kmer_count(sequences[i][::-1], k=k, step=1))\n",
    "    \n",
    "v = DictVectorizer(sparse=False)\n",
    "feature_values = v.fit_transform(kmers_count)\n",
    "feature_names = v.get_feature_names_out()\n",
    "X_inv = pd.DataFrame(feature_values, columns=feature_names)\n",
    "Q = X_inv.to_numpy()\n",
    "\n",
    "alpha = 0.7\n",
    "beta = 0.5\n",
    "feature_mask_2 = MIM_Feature_Selector(P, Q, alpha, beta)\n",
    "print(f\"Feature before: {len(X.columns.values)} - Feature After: {np.sum(feature_mask_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d218466-d6c2-41ec-aed4-03477a5cf002",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, max_features=8, max_depth=16)\n",
    "assess_model_on_full_features(model)\n",
    "assess_model_on_selected_features(feature_mask_2, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7266a75-bdbb-4e77-938d-66338bb98c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING RANDOM BACKGROUND SELECTION\n",
    "\n",
    "from itertools import product\n",
    "def calculate_nucleotide_probabilities(sequences, k):\n",
    "    kmers_count = []\n",
    "    for seq in sequences:\n",
    "        nucleotide_counts = {'A': 0, 'C': 0, 'G': 0, 'T': 0}\n",
    "        total_nucleotides = 0\n",
    "        kmers = {}\n",
    "        \n",
    "        for nucleotide in seq:\n",
    "            if nucleotide in nucleotide_counts:\n",
    "                nucleotide_counts[nucleotide] += 1\n",
    "                total_nucleotides += 1\n",
    "    \n",
    "        probabilities = {nucleotide: count / total_nucleotides for nucleotide, count in nucleotide_counts.items()}\n",
    "        nucleotides = list(probabilities.keys())\n",
    "    \n",
    "        for kmer in product(nucleotides, repeat=k):\n",
    "            kmer_prob = np.prod([probabilities[n] for n in kmer])\n",
    "            kmers[''.join(kmer)] = kmer_prob\n",
    "        \n",
    "        kmers_count.append(kmers)\n",
    "    return kmers_count\n",
    "        \n",
    "kmers_count = calculate_nucleotide_probabilities(sequences, k)\n",
    "\n",
    "v = DictVectorizer(sparse=False)\n",
    "feature_values = v.fit_transform(kmers_count)\n",
    "feature_names = v.get_feature_names_out()\n",
    "X_inv = pd.DataFrame(feature_values, columns=feature_names)\n",
    "Q = X_inv.to_numpy()\n",
    "\n",
    "alpha = 0.7\n",
    "beta = 0.5\n",
    "feature_mask_3 = MIM_Feature_Selector(P, Q, alpha, beta)\n",
    "print(f\"Feature before: {len(X.columns.values)} - Feature After: {np.sum(feature_mask_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3c94ad-6798-450e-8417-bc4aa7785890",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, max_features=8, max_depth=16)\n",
    "assess_model_on_full_features(model)\n",
    "assess_model_on_selected_features(feature_mask_3, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d16a7f-0d52-43a5-8b1c-608f5abff165",
   "metadata": {},
   "source": [
    "<h4 style=\"background-color: #3ab76f; border-top: 4px solid #dddddd; display: flex; color: white;\">\n",
    "    <ul><li>RESULT</li></ul>\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f996d97-34ca-42ac-abe1-5f63f5f821a1",
   "metadata": {},
   "source": [
    "* Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e7709e-4850-4440-a76e-dc072d2bfea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(k):\n",
    "    sequences   = dataset['sequence']\n",
    "    kmers_count = []\n",
    "    for i in range(len(sequences)):\n",
    "        kmers_count.append(DNA.kmer_count(sequences[i], k=k, step=1))\n",
    "    v = DictVectorizer(sparse=False)\n",
    "    feature_values = v.fit_transform(kmers_count)\n",
    "    feature_names = v.get_feature_names_out()\n",
    "    X = pd.DataFrame(feature_values, columns=feature_names)\n",
    "    y = dataset['class']\n",
    "    return X,y\n",
    "\n",
    "def tune_model(model_conf, X, y):\n",
    "    best_models = []\n",
    "    for config in model_conf:\n",
    "        name = config['name']\n",
    "        model = config['model']\n",
    "        params = config['params']\n",
    "        \n",
    "        print(f\"Tuning {name}\")\n",
    "        if params:\n",
    "            grid_search = GridSearchCV(estimator=model, param_grid=params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "            grid_search.fit(X, y)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            best_params = grid_search.best_params_\n",
    "            print(f\"Best params for {name}: {best_params}\")\n",
    "        else:\n",
    "            best_model = model\n",
    "            print(f\"No hyperparameters to tune for {name}\")\n",
    "    \n",
    "        best_models.append({\n",
    "            'name': name,\n",
    "            'model': best_model,\n",
    "        })\n",
    "    return best_models\n",
    "    \n",
    "def trace_model_performance(traced_performance, tuned_models, feature_mask, feature_selection_method, X, y, k_mer_size, out=\"\"):\n",
    "    for model_info in tuned_models:\n",
    "        name = model_info['name']\n",
    "        model = model_info['model']\n",
    "        \n",
    "        # Apply the feature mask if provided\n",
    "        feature_name = \"-\".join(X.columns.values)\n",
    "        if feature_mask is not None:\n",
    "            mask_series = pd.Series(feature_mask, index=X.columns)\n",
    "            selected_features = mask_series[mask_series].index\n",
    "            X_train = X[selected_features]\n",
    "            X_test  = X_test_glob[selected_features]\n",
    "            feature_name = \"-\".join(selected_features)\n",
    "        else:\n",
    "            X_train = X\n",
    "            X_test = X_test_glob\n",
    "\n",
    "        print(f\"Evaluating {name} with feature selection method: {feature_selection_method}\")\n",
    "        y_train = y\n",
    "        y_test = y_test_glob\n",
    "\n",
    "        # Perform cross-validation\n",
    "        kfold = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "        cv_results = cross_validate(model, X_train, y_train, cv=kfold, scoring=['accuracy', 'precision', 'recall', 'f1'])\n",
    "        best_score = np.mean(cv_results['test_accuracy'])\n",
    "        std_score = np.std(cv_results['test_accuracy'])\n",
    "        precision = np.mean(cv_results['test_precision'])\n",
    "        recall = np.mean(cv_results['test_recall'])\n",
    "        f1 = np.mean(cv_results['test_f1'])\n",
    "\n",
    "        # Fit the best model\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        traced_performance.append({\n",
    "            'name': name,\n",
    "            'model': model,\n",
    "            'test_score': accuracy_score(y_test, predictions),\n",
    "            'train_score': best_score,\n",
    "            'train_std_score': \"±\"+str(std_score),\n",
    "            'train_precision': precision,\n",
    "            'train_recall': recall,\n",
    "            'train_f1': f1,\n",
    "            'feature_selection_method': feature_selection_method,\n",
    "            'k-mer size': k_mer_size,\n",
    "            'feature_mask': feature_name\n",
    "        })\n",
    "\n",
    "        # SAVE RESULT\n",
    "        df_performance = pd.DataFrame(traced_performance)\n",
    "        df_performance.to_csv(out, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e0922c-04b1-4c27-8cbf-028428b87bc7",
   "metadata": {},
   "source": [
    "* Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd012fc1-c0da-4a93-9fc0-00d92bb8a890",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conf = [\n",
    "    {'name':'GaussianProcessClassifier', 'model':GaussianProcessClassifier(), \"params\": {'kernel': [1.0 * RBF(1.0), 1.0 * RBF(0.5), 1.0 * RBF(2.0)]}},\n",
    "    {'name':'RandomForestClassifier', 'model':RandomForestClassifier(), \"params\": {'max_depth': [8, 16, 32, 64, 128], 'n_estimators': [10, 50, 100, 200, 300], 'max_features': [2, 4, 8, 16]}},\n",
    "    {'name':'AdaBoostClassifier', 'model':AdaBoostClassifier(algorithm=\"SAMME\"), \"params\": {'n_estimators': [50, 100, 200]}},\n",
    "    {'name':'GaussianNB', 'model':GaussianNB(), \"params\": None},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a796ce10-2c1c-48bc-9534-69b36c0c59cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = prepare_dataset(k)\n",
    "traced_performance = []\n",
    "threshold = 0.25\n",
    "path_result=f'../Output/FSelect/result_k_{k}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c058f353-25b4-4140-a693-00c71592c736",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_models = tune_model(model_conf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b379c011-038f-4c71-8271-34a3dd63356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5079fcd-b984-416d-a390-7b82c9b75861",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_model_performance(traced_performance, tuned_models, None, \"No\", X, y, k, out=path_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d11832e-15c9-4759-9830-afaa7dff3c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_model_performance(traced_performance, tuned_models, None, \"Variance-based(1)\", X, y, k, out=path_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c1afe8-377b-4c40-9a4e-18ebcab578c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_model_performance(traced_performance, tuned_models, fs['FTest']['select'][threshold], \"FTest\", X, y, k, out=path_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bc0f92-a8df-480b-9417-372de540a0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_model_performance(traced_performance, tuned_models, fs['RandomForest']['select'][threshold], \"RandomForest\", X, y, k, out=path_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51bf301-ded9-490e-9254-65aae712be72",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_model_performance(traced_performance, tuned_models, fs['Lasso']['select'][threshold], \"Lasso\", X, y, k, out=path_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe991ae-b81e-4a94-816b-b4723590eaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_model_performance(traced_performance, tuned_models, fs['MutualInformation']['select'][threshold], \"MutualInformation\", X, y, k, out=path_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ff4e6c-7902-4192-8a14-1535f5944961",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_model_performance(traced_performance, tuned_models, feature_mask_1, \"MIM-FULL\", X, y, k, out=path_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed815f59-2e34-48f8-808b-10bc857ce39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_model_performance(traced_performance, tuned_models, feature_mask_2, \"MIM-NB\", X, y, k, out=path_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f78ebd-e872-4f81-830d-5997b846673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_model_performance(traced_performance, tuned_models, feature_mask_3, \"MIM-RB\", X, y, k, out=path_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298a4b2c-b66f-4153-9152-db2443a0f633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIEW RESULT\n",
    "df_performance = pd.read_csv(path_result)\n",
    "#df_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b54653-936f-4cb4-90ac-080d37246531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4d2e5a-da89-4a7a-b646-b70756919fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
